{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from models import *\n",
    "from def_dict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 216, 64, 3), (None, 216, 64, 3)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/w58y67/step_2/'\n",
    "\n",
    "x_train = np.load(data_path+'x_train.npy')\n",
    "x_train = x_train/255\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, x_train)).shuffle(10000).batch(256)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, vae = build_vae(x_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 216, 64, 3)]      0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  1860052   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 216, 64, 3)        1864355   \n",
      "=================================================================\n",
      "Total params: 3,724,407\n",
      "Trainable params: 3,724,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "\n",
    "def get_rec_loss(inputs, predictions):\n",
    "    rec_loss = tf.keras.losses.binary_crossentropy(inputs, predictions)\n",
    "    rec_loss = tf.reduce_mean(rec_loss)\n",
    "    rec_loss *= 200*48\n",
    "    return rec_loss\n",
    "\n",
    "def get_kl_loss(z_log_var, z_mean):\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Get model ouputs\n",
    "        z_log_var, z_mean, z = encoder(inputs)\n",
    "        predictions = decoder(z)\n",
    "        \n",
    "        # Compute losses\n",
    "        rec_loss = get_rec_loss(inputs, predictions)\n",
    "        kl_loss = get_kl_loss(z_log_var, z_mean)\n",
    "        loss = rec_loss + kl_loss\n",
    "    \n",
    "    # Compute gradients\n",
    "    varialbes = vae.trainable_variables\n",
    "    gradients = tape.gradient(loss, varialbes)\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, varialbes))\n",
    "    \n",
    "    # Update train loss\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'train_result/train_1/'\n",
    "if_not_make(save_dir)\n",
    "\n",
    "def reduce_lr(pre_v_loss, v_loss, count, lr, patience, factor, min_lr):\n",
    "    if v_loss < pre_v_loss:\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        if count >= patience: \n",
    "            lr = lr*factor\n",
    "            if lr < min_lr: \n",
    "                lr = min_lr\n",
    "            count = 0\n",
    "            print('reduce learning rate..', lr)    \n",
    "    return count, lr\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder, decoder=decoder, vae=vae)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(save_dir+'/training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f82e9a4e790>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint('train_result/train_1/ckp/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 * loss: 6102.498535,  best_loss: 6102.498535, l_rate: 0.001000, lr_count: 0\n",
      "* 1 * loss: 5815.026855,  best_loss: 5815.026855, l_rate: 0.001000, lr_count: 0\n",
      "* 2 * loss: 5755.753418,  best_loss: 5755.753418, l_rate: 0.001000, lr_count: 0\n",
      "* 3 * loss: 5730.976074,  best_loss: 5730.976074, l_rate: 0.001000, lr_count: 0\n",
      "* 4 * loss: 5721.758301,  best_loss: 5721.758301, l_rate: 0.001000, lr_count: 0\n",
      "* 5 * loss: 5716.935547,  best_loss: 5716.935547, l_rate: 0.001000, lr_count: 0\n",
      "* 6 * loss: 5713.729980,  best_loss: 5713.729980, l_rate: 0.001000, lr_count: 0\n",
      "* 7 * loss: 5711.673340,  best_loss: 5711.673340, l_rate: 0.001000, lr_count: 0\n",
      "* 8 * loss: 5709.840332,  best_loss: 5709.840332, l_rate: 0.001000, lr_count: 0\n",
      "* 9 * loss: 5708.485352,  best_loss: 5708.485352, l_rate: 0.001000, lr_count: 0\n",
      "* 10 * loss: 5707.375000,  best_loss: 5707.375000, l_rate: 0.001000, lr_count: 0\n",
      "* 11 * loss: 5706.532715,  best_loss: 5706.532715, l_rate: 0.001000, lr_count: 0\n",
      "* 12 * loss: 5705.679199,  best_loss: 5705.679199, l_rate: 0.001000, lr_count: 0\n",
      "* 13 * loss: 5705.118652,  best_loss: 5705.118652, l_rate: 0.001000, lr_count: 0\n",
      "* 14 * loss: 5704.373535,  best_loss: 5704.373535, l_rate: 0.001000, lr_count: 0\n",
      "* 15 * loss: 5703.931641,  best_loss: 5703.931641, l_rate: 0.001000, lr_count: 0\n",
      "* 16 * loss: 5703.474121,  best_loss: 5703.474121, l_rate: 0.001000, lr_count: 0\n",
      "* 17 * loss: 5702.953613,  best_loss: 5702.953613, l_rate: 0.001000, lr_count: 0\n",
      "* 18 * loss: 5702.551758,  best_loss: 5702.551758, l_rate: 0.001000, lr_count: 0\n",
      "* 19 * loss: 5702.225586,  best_loss: 5702.225586, l_rate: 0.001000, lr_count: 0\n",
      "* 20 * loss: 5701.939941,  best_loss: 5701.939941, l_rate: 0.001000, lr_count: 0\n",
      "* 21 * loss: 5701.520020,  best_loss: 5701.520020, l_rate: 0.001000, lr_count: 0\n",
      "* 22 * loss: 5701.139160,  best_loss: 5701.139160, l_rate: 0.001000, lr_count: 0\n",
      "* 23 * loss: 5700.877441,  best_loss: 5700.877441, l_rate: 0.001000, lr_count: 0\n",
      "* 24 * loss: 5700.592773,  best_loss: 5700.592773, l_rate: 0.001000, lr_count: 0\n",
      "* 25 * loss: 5700.203613,  best_loss: 5700.203613, l_rate: 0.001000, lr_count: 0\n",
      "* 26 * loss: 5700.081055,  best_loss: 5700.081055, l_rate: 0.001000, lr_count: 0\n",
      "* 27 * loss: 5699.719727,  best_loss: 5699.719727, l_rate: 0.001000, lr_count: 0\n",
      "* 28 * loss: 5699.469727,  best_loss: 5699.469727, l_rate: 0.001000, lr_count: 0\n",
      "* 29 * loss: 5699.214355,  best_loss: 5699.214355, l_rate: 0.001000, lr_count: 0\n",
      "* 30 * loss: 5699.107422,  best_loss: 5699.107422, l_rate: 0.001000, lr_count: 0\n",
      "* 31 * loss: 5698.822266,  best_loss: 5698.822266, l_rate: 0.001000, lr_count: 0\n",
      "* 32 * loss: 5698.606934,  best_loss: 5698.606934, l_rate: 0.001000, lr_count: 0\n",
      "* 33 * loss: 5698.531738,  best_loss: 5698.531738, l_rate: 0.001000, lr_count: 0\n",
      "* 34 * loss: 5698.241699,  best_loss: 5698.241699, l_rate: 0.001000, lr_count: 0\n",
      "* 35 * loss: 5698.122559,  best_loss: 5698.122559, l_rate: 0.001000, lr_count: 0\n",
      "* 36 * loss: 5697.955566,  best_loss: 5697.955566, l_rate: 0.001000, lr_count: 0\n",
      "* 37 * loss: 5697.782715,  best_loss: 5697.782715, l_rate: 0.001000, lr_count: 0\n",
      "* 38 * loss: 5697.636719,  best_loss: 5697.636719, l_rate: 0.001000, lr_count: 0\n",
      "* 39 * loss: 5697.526855,  best_loss: 5697.526855, l_rate: 0.001000, lr_count: 0\n",
      "* 40 * loss: 5697.387695,  best_loss: 5697.387695, l_rate: 0.001000, lr_count: 0\n",
      "* 41 * loss: 5697.203125,  best_loss: 5697.203125, l_rate: 0.001000, lr_count: 0\n",
      "* 42 * loss: 5697.054688,  best_loss: 5697.054688, l_rate: 0.001000, lr_count: 0\n",
      "* 43 * loss: 5696.945312,  best_loss: 5696.945312, l_rate: 0.001000, lr_count: 0\n",
      "* 44 * loss: 5696.808105,  best_loss: 5696.808105, l_rate: 0.001000, lr_count: 0\n",
      "* 45 * loss: 5696.771973,  best_loss: 5696.771973, l_rate: 0.001000, lr_count: 0\n",
      "* 46 * loss: 5696.579102,  best_loss: 5696.579102, l_rate: 0.001000, lr_count: 0\n",
      "* 47 * loss: 5696.534668,  best_loss: 5696.534668, l_rate: 0.001000, lr_count: 0\n",
      "* 48 * loss: 5696.366211,  best_loss: 5696.366211, l_rate: 0.001000, lr_count: 0\n",
      "* 49 * loss: 5696.299805,  best_loss: 5696.299805, l_rate: 0.001000, lr_count: 0\n",
      "* 50 * loss: 5696.281738,  best_loss: 5696.281738, l_rate: 0.001000, lr_count: 0\n",
      "* 51 * loss: 5696.185059,  best_loss: 5696.185059, l_rate: 0.001000, lr_count: 0\n",
      "* 52 * loss: 5696.099121,  best_loss: 5696.099121, l_rate: 0.001000, lr_count: 0\n",
      "* 53 * loss: 5695.947266,  best_loss: 5695.947266, l_rate: 0.001000, lr_count: 0\n",
      "* 54 * loss: 5695.857422,  best_loss: 5695.857422, l_rate: 0.001000, lr_count: 0\n",
      "* 55 * loss: 5695.764160,  best_loss: 5695.764160, l_rate: 0.001000, lr_count: 0\n",
      "* 56 * loss: 5695.664062,  best_loss: 5695.664062, l_rate: 0.001000, lr_count: 0\n",
      "* 57 * loss: 5695.690918,  best_loss: 5695.664062, l_rate: 0.001000, lr_count: 1\n",
      "* 58 * loss: 5695.608398,  best_loss: 5695.608398, l_rate: 0.001000, lr_count: 0\n",
      "* 59 * loss: 5695.532227,  best_loss: 5695.532227, l_rate: 0.001000, lr_count: 0\n",
      "* 60 * loss: 5695.371582,  best_loss: 5695.371582, l_rate: 0.001000, lr_count: 0\n",
      "* 61 * loss: 5695.347168,  best_loss: 5695.347168, l_rate: 0.001000, lr_count: 0\n",
      "* 62 * loss: 5695.266602,  best_loss: 5695.266602, l_rate: 0.001000, lr_count: 0\n",
      "* 63 * loss: 5695.187500,  best_loss: 5695.187500, l_rate: 0.001000, lr_count: 0\n",
      "* 64 * loss: 5695.187012,  best_loss: 5695.187012, l_rate: 0.001000, lr_count: 0\n",
      "* 65 * loss: 5695.059082,  best_loss: 5695.059082, l_rate: 0.001000, lr_count: 0\n",
      "* 66 * loss: 5694.982422,  best_loss: 5694.982422, l_rate: 0.001000, lr_count: 0\n",
      "* 67 * loss: 5694.931152,  best_loss: 5694.931152, l_rate: 0.001000, lr_count: 0\n",
      "* 68 * loss: 5694.945312,  best_loss: 5694.931152, l_rate: 0.001000, lr_count: 1\n",
      "* 69 * loss: 5694.810059,  best_loss: 5694.810059, l_rate: 0.001000, lr_count: 0\n",
      "* 70 * loss: 5694.743652,  best_loss: 5694.743652, l_rate: 0.001000, lr_count: 0\n",
      "* 71 * loss: 5694.689453,  best_loss: 5694.689453, l_rate: 0.001000, lr_count: 0\n",
      "* 72 * loss: 5694.641602,  best_loss: 5694.641602, l_rate: 0.001000, lr_count: 0\n",
      "* 73 * loss: 5694.633301,  best_loss: 5694.633301, l_rate: 0.001000, lr_count: 0\n",
      "* 74 * loss: 5694.482422,  best_loss: 5694.482422, l_rate: 0.001000, lr_count: 0\n",
      "* 75 * loss: 5694.486328,  best_loss: 5694.482422, l_rate: 0.001000, lr_count: 1\n",
      "* 76 * loss: 5694.378418,  best_loss: 5694.378418, l_rate: 0.001000, lr_count: 0\n",
      "* 77 * loss: 5694.395508,  best_loss: 5694.378418, l_rate: 0.001000, lr_count: 1\n",
      "* 78 * loss: 5694.311035,  best_loss: 5694.311035, l_rate: 0.001000, lr_count: 0\n",
      "* 79 * loss: 5694.305176,  best_loss: 5694.305176, l_rate: 0.001000, lr_count: 0\n",
      "* 80 * loss: 5694.239258,  best_loss: 5694.239258, l_rate: 0.001000, lr_count: 0\n",
      "* 81 * loss: 5694.203613,  best_loss: 5694.203613, l_rate: 0.001000, lr_count: 0\n",
      "* 82 * loss: 5694.167480,  best_loss: 5694.167480, l_rate: 0.001000, lr_count: 0\n",
      "* 83 * loss: 5694.150879,  best_loss: 5694.150879, l_rate: 0.001000, lr_count: 0\n",
      "* 84 * loss: 5694.033691,  best_loss: 5694.033691, l_rate: 0.001000, lr_count: 0\n",
      "* 85 * loss: 5694.040527,  best_loss: 5694.033691, l_rate: 0.001000, lr_count: 1\n",
      "* 86 * loss: 5693.946289,  best_loss: 5693.946289, l_rate: 0.001000, lr_count: 0\n",
      "* 87 * loss: 5693.926758,  best_loss: 5693.926758, l_rate: 0.001000, lr_count: 0\n",
      "* 88 * loss: 5693.814453,  best_loss: 5693.814453, l_rate: 0.001000, lr_count: 0\n",
      "* 89 * loss: 5693.832031,  best_loss: 5693.814453, l_rate: 0.001000, lr_count: 1\n",
      "* 90 * loss: 5693.816406,  best_loss: 5693.814453, l_rate: 0.001000, lr_count: 2\n",
      "* 91 * loss: 5693.757812,  best_loss: 5693.757812, l_rate: 0.001000, lr_count: 0\n",
      "* 92 * loss: 5693.685059,  best_loss: 5693.685059, l_rate: 0.001000, lr_count: 0\n",
      "* 93 * loss: 5693.692871,  best_loss: 5693.685059, l_rate: 0.001000, lr_count: 1\n",
      "* 94 * loss: 5693.616699,  best_loss: 5693.616699, l_rate: 0.001000, lr_count: 0\n",
      "* 95 * loss: 5693.593262,  best_loss: 5693.593262, l_rate: 0.001000, lr_count: 0\n",
      "* 96 * loss: 5693.562500,  best_loss: 5693.562500, l_rate: 0.001000, lr_count: 0\n",
      "* 97 * loss: 5693.516113,  best_loss: 5693.516113, l_rate: 0.001000, lr_count: 0\n",
      "* 98 * loss: 5693.570801,  best_loss: 5693.516113, l_rate: 0.001000, lr_count: 1\n",
      "* 99 * loss: 5693.486328,  best_loss: 5693.486328, l_rate: 0.001000, lr_count: 0\n",
      "* 100 * loss: 5693.403809,  best_loss: 5693.403809, l_rate: 0.001000, lr_count: 0\n",
      "* 101 * loss: 5693.430176,  best_loss: 5693.403809, l_rate: 0.001000, lr_count: 1\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "# Initialize values\n",
    "best_loss, count = float('inf'), 0\n",
    "\n",
    "# Start epoch loop\n",
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in train_ds:\n",
    "        train_step(inputs)\n",
    "    \n",
    "    # Get loss and leraning rate at this epoch\n",
    "    t_loss = train_loss.result().numpy() \n",
    "    l_rate = optimizer.learning_rate.numpy()\n",
    "\n",
    "    # Control learning rate\n",
    "    count, lr  = reduce_lr(best_loss, t_loss, count, l_rate, 5, 0.2, 0.00001)\n",
    "    optimizer.learning_rate = lr\n",
    "    \n",
    "    # Save checkpoint if best v_loss \n",
    "    if t_loss < best_loss:\n",
    "        best_loss = t_loss\n",
    "        checkpoint.save(file_prefix=os.path.join(save_dir+'/ckp/', 'ckp'))\n",
    "    \n",
    "    # Save loss, lerning rate\n",
    "    print(\"* %i * loss: %f,  best_loss: %f, l_rate: %f, lr_count: %i\"%(epoch, t_loss, best_loss, l_rate, count ))\n",
    "    df = pd.DataFrame({'epoch':[epoch], 'loss':[t_loss], 'best_loss':[best_loss], 'l_rate':[l_rate]  } )\n",
    "    df.to_csv(save_dir+'/process.csv', mode='a', header=False)\n",
    "    \n",
    "    # Reset loss\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
