{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from models import *\n",
    "from def_dict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 200, 48, 3), (None, 200, 48, 3)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/w58y67_prepro/'\n",
    "\n",
    "x_train = np.load(data_path+'x_train.npy')\n",
    "x_train = x_train/255\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, x_train)).shuffle(10000).batch(256)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, vae = build_vae(x_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200, 48, 3)]      0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  1319380   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 200, 48, 3)        1321571   \n",
      "=================================================================\n",
      "Total params: 2,640,951\n",
      "Trainable params: 2,640,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbc3e6e6bd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(data_path+'/train_1/ckp/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "\n",
    "def get_rec_loss(inputs, predictions):\n",
    "    rec_loss = tf.keras.losses.binary_crossentropy(inputs, predictions)\n",
    "    rec_loss = tf.reduce_mean(rec_loss)\n",
    "    rec_loss *= 200*48\n",
    "    return rec_loss\n",
    "\n",
    "def get_kl_loss(z_log_var, z_mean):\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Get model ouputs\n",
    "        z_log_var, z_mean, z = encoder(inputs)\n",
    "        predictions = decoder(z)\n",
    "        \n",
    "        # Compute losses\n",
    "        rec_loss = get_rec_loss(inputs, predictions)\n",
    "        kl_loss = get_kl_loss(z_log_var, z_mean)\n",
    "        loss = rec_loss + kl_loss\n",
    "    \n",
    "    # Compute gradients\n",
    "    varialbes = vae.trainable_variables\n",
    "    gradients = tape.gradient(loss, varialbes)\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, varialbes))\n",
    "    \n",
    "    # Update train loss\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'data/w58y67_prepro/train_2/'\n",
    "if_not_make(save_dir)\n",
    "\n",
    "def reduce_lr(pre_v_loss, v_loss, count, lr, patience, factor, min_lr):\n",
    "    if v_loss < pre_v_loss:\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        if count >= patience: \n",
    "            lr = lr*factor\n",
    "            if lr < min_lr: \n",
    "                lr = min_lr\n",
    "            count = 0\n",
    "            print('reduce learning rate..', lr)    \n",
    "    return count, lr\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder, decoder=decoder, vae=vae)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(save_dir+'/training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 * loss: 5689.645020,  best_loss: 5689.645020, l_rate: 0.001000, lr_count: 0\n",
      "* 1 * loss: 5689.657227,  best_loss: 5689.645020, l_rate: 0.001000, lr_count: 1\n",
      "* 2 * loss: 5689.658203,  best_loss: 5689.645020, l_rate: 0.001000, lr_count: 2\n",
      "* 3 * loss: 5689.668457,  best_loss: 5689.645020, l_rate: 0.001000, lr_count: 3\n",
      "* 4 * loss: 5689.664062,  best_loss: 5689.645020, l_rate: 0.001000, lr_count: 4\n",
      "reduce learning rate.. 0.00020000000949949026\n",
      "* 5 * loss: 5689.674805,  best_loss: 5689.645020, l_rate: 0.001000, lr_count: 0\n",
      "* 6 * loss: 5688.839355,  best_loss: 5688.839355, l_rate: 0.000200, lr_count: 0\n",
      "* 7 * loss: 5688.720215,  best_loss: 5688.720215, l_rate: 0.000200, lr_count: 0\n",
      "* 8 * loss: 5688.690430,  best_loss: 5688.690430, l_rate: 0.000200, lr_count: 0\n",
      "* 9 * loss: 5688.682617,  best_loss: 5688.682617, l_rate: 0.000200, lr_count: 0\n",
      "* 10 * loss: 5688.670410,  best_loss: 5688.670410, l_rate: 0.000200, lr_count: 0\n",
      "* 11 * loss: 5688.652832,  best_loss: 5688.652832, l_rate: 0.000200, lr_count: 0\n",
      "* 12 * loss: 5688.660645,  best_loss: 5688.652832, l_rate: 0.000200, lr_count: 1\n",
      "* 13 * loss: 5688.637207,  best_loss: 5688.637207, l_rate: 0.000200, lr_count: 0\n",
      "* 14 * loss: 5688.628418,  best_loss: 5688.628418, l_rate: 0.000200, lr_count: 0\n",
      "* 15 * loss: 5688.629883,  best_loss: 5688.628418, l_rate: 0.000200, lr_count: 1\n",
      "* 16 * loss: 5688.640625,  best_loss: 5688.628418, l_rate: 0.000200, lr_count: 2\n",
      "* 17 * loss: 5688.631836,  best_loss: 5688.628418, l_rate: 0.000200, lr_count: 3\n",
      "* 18 * loss: 5688.620117,  best_loss: 5688.620117, l_rate: 0.000200, lr_count: 0\n",
      "* 19 * loss: 5688.622559,  best_loss: 5688.620117, l_rate: 0.000200, lr_count: 1\n",
      "* 20 * loss: 5688.622559,  best_loss: 5688.620117, l_rate: 0.000200, lr_count: 2\n",
      "* 21 * loss: 5688.623535,  best_loss: 5688.620117, l_rate: 0.000200, lr_count: 3\n",
      "* 22 * loss: 5688.613281,  best_loss: 5688.613281, l_rate: 0.000200, lr_count: 0\n",
      "* 23 * loss: 5688.619141,  best_loss: 5688.613281, l_rate: 0.000200, lr_count: 1\n",
      "* 24 * loss: 5688.597168,  best_loss: 5688.597168, l_rate: 0.000200, lr_count: 0\n",
      "* 25 * loss: 5688.614258,  best_loss: 5688.597168, l_rate: 0.000200, lr_count: 1\n",
      "* 26 * loss: 5688.593750,  best_loss: 5688.593750, l_rate: 0.000200, lr_count: 0\n",
      "* 27 * loss: 5688.591797,  best_loss: 5688.591797, l_rate: 0.000200, lr_count: 0\n",
      "* 28 * loss: 5688.589844,  best_loss: 5688.589844, l_rate: 0.000200, lr_count: 0\n",
      "* 29 * loss: 5688.573242,  best_loss: 5688.573242, l_rate: 0.000200, lr_count: 0\n",
      "* 30 * loss: 5688.590332,  best_loss: 5688.573242, l_rate: 0.000200, lr_count: 1\n",
      "* 31 * loss: 5688.588867,  best_loss: 5688.573242, l_rate: 0.000200, lr_count: 2\n",
      "* 32 * loss: 5688.581543,  best_loss: 5688.573242, l_rate: 0.000200, lr_count: 3\n",
      "* 33 * loss: 5688.572266,  best_loss: 5688.572266, l_rate: 0.000200, lr_count: 0\n",
      "* 34 * loss: 5688.574707,  best_loss: 5688.572266, l_rate: 0.000200, lr_count: 1\n",
      "* 35 * loss: 5688.564941,  best_loss: 5688.564941, l_rate: 0.000200, lr_count: 0\n",
      "* 36 * loss: 5688.546387,  best_loss: 5688.546387, l_rate: 0.000200, lr_count: 0\n",
      "* 37 * loss: 5688.551270,  best_loss: 5688.546387, l_rate: 0.000200, lr_count: 1\n",
      "* 38 * loss: 5688.559082,  best_loss: 5688.546387, l_rate: 0.000200, lr_count: 2\n",
      "* 39 * loss: 5688.541504,  best_loss: 5688.541504, l_rate: 0.000200, lr_count: 0\n",
      "* 40 * loss: 5688.556152,  best_loss: 5688.541504, l_rate: 0.000200, lr_count: 1\n",
      "* 41 * loss: 5688.547363,  best_loss: 5688.541504, l_rate: 0.000200, lr_count: 2\n",
      "* 42 * loss: 5688.537598,  best_loss: 5688.537598, l_rate: 0.000200, lr_count: 0\n",
      "* 43 * loss: 5688.539551,  best_loss: 5688.537598, l_rate: 0.000200, lr_count: 1\n",
      "* 44 * loss: 5688.533691,  best_loss: 5688.533691, l_rate: 0.000200, lr_count: 0\n",
      "* 45 * loss: 5688.530762,  best_loss: 5688.530762, l_rate: 0.000200, lr_count: 0\n",
      "* 46 * loss: 5688.531738,  best_loss: 5688.530762, l_rate: 0.000200, lr_count: 1\n",
      "* 47 * loss: 5688.523926,  best_loss: 5688.523926, l_rate: 0.000200, lr_count: 0\n",
      "* 48 * loss: 5688.532715,  best_loss: 5688.523926, l_rate: 0.000200, lr_count: 1\n",
      "* 49 * loss: 5688.511230,  best_loss: 5688.511230, l_rate: 0.000200, lr_count: 0\n",
      "* 50 * loss: 5688.508789,  best_loss: 5688.508789, l_rate: 0.000200, lr_count: 0\n",
      "* 51 * loss: 5688.501465,  best_loss: 5688.501465, l_rate: 0.000200, lr_count: 0\n",
      "* 52 * loss: 5688.520508,  best_loss: 5688.501465, l_rate: 0.000200, lr_count: 1\n",
      "* 53 * loss: 5688.510254,  best_loss: 5688.501465, l_rate: 0.000200, lr_count: 2\n",
      "* 54 * loss: 5688.491211,  best_loss: 5688.491211, l_rate: 0.000200, lr_count: 0\n",
      "* 55 * loss: 5688.525391,  best_loss: 5688.491211, l_rate: 0.000200, lr_count: 1\n",
      "* 56 * loss: 5688.492676,  best_loss: 5688.491211, l_rate: 0.000200, lr_count: 2\n",
      "* 57 * loss: 5688.498535,  best_loss: 5688.491211, l_rate: 0.000200, lr_count: 3\n",
      "* 58 * loss: 5688.492676,  best_loss: 5688.491211, l_rate: 0.000200, lr_count: 4\n",
      "reduce learning rate.. 4.0000001899898055e-05\n",
      "* 59 * loss: 5688.503418,  best_loss: 5688.491211, l_rate: 0.000200, lr_count: 0\n",
      "* 60 * loss: 5688.300781,  best_loss: 5688.300781, l_rate: 0.000040, lr_count: 0\n",
      "* 61 * loss: 5688.281250,  best_loss: 5688.281250, l_rate: 0.000040, lr_count: 0\n",
      "* 62 * loss: 5688.267090,  best_loss: 5688.267090, l_rate: 0.000040, lr_count: 0\n",
      "* 63 * loss: 5688.256836,  best_loss: 5688.256836, l_rate: 0.000040, lr_count: 0\n",
      "* 64 * loss: 5688.259766,  best_loss: 5688.256836, l_rate: 0.000040, lr_count: 1\n",
      "* 65 * loss: 5688.256348,  best_loss: 5688.256348, l_rate: 0.000040, lr_count: 0\n",
      "* 66 * loss: 5688.263184,  best_loss: 5688.256348, l_rate: 0.000040, lr_count: 1\n",
      "* 67 * loss: 5688.247559,  best_loss: 5688.247559, l_rate: 0.000040, lr_count: 0\n",
      "* 68 * loss: 5688.250977,  best_loss: 5688.247559, l_rate: 0.000040, lr_count: 1\n",
      "* 69 * loss: 5688.249512,  best_loss: 5688.247559, l_rate: 0.000040, lr_count: 2\n",
      "* 70 * loss: 5688.246094,  best_loss: 5688.246094, l_rate: 0.000040, lr_count: 0\n",
      "* 71 * loss: 5688.248535,  best_loss: 5688.246094, l_rate: 0.000040, lr_count: 1\n",
      "* 72 * loss: 5688.244141,  best_loss: 5688.244141, l_rate: 0.000040, lr_count: 0\n",
      "* 73 * loss: 5688.244629,  best_loss: 5688.244141, l_rate: 0.000040, lr_count: 1\n",
      "* 74 * loss: 5688.251465,  best_loss: 5688.244141, l_rate: 0.000040, lr_count: 2\n",
      "* 75 * loss: 5688.247070,  best_loss: 5688.244141, l_rate: 0.000040, lr_count: 3\n",
      "* 76 * loss: 5688.242676,  best_loss: 5688.242676, l_rate: 0.000040, lr_count: 0\n",
      "* 77 * loss: 5688.244629,  best_loss: 5688.242676, l_rate: 0.000040, lr_count: 1\n",
      "* 78 * loss: 5688.240234,  best_loss: 5688.240234, l_rate: 0.000040, lr_count: 0\n",
      "* 79 * loss: 5688.245605,  best_loss: 5688.240234, l_rate: 0.000040, lr_count: 1\n",
      "* 80 * loss: 5688.231445,  best_loss: 5688.231445, l_rate: 0.000040, lr_count: 0\n",
      "* 81 * loss: 5688.230957,  best_loss: 5688.230957, l_rate: 0.000040, lr_count: 0\n",
      "* 82 * loss: 5688.246582,  best_loss: 5688.230957, l_rate: 0.000040, lr_count: 1\n",
      "* 83 * loss: 5688.236816,  best_loss: 5688.230957, l_rate: 0.000040, lr_count: 2\n",
      "* 84 * loss: 5688.236328,  best_loss: 5688.230957, l_rate: 0.000040, lr_count: 3\n",
      "* 85 * loss: 5688.235840,  best_loss: 5688.230957, l_rate: 0.000040, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 86 * loss: 5688.235840,  best_loss: 5688.230957, l_rate: 0.000040, lr_count: 0\n",
      "* 87 * loss: 5688.205078,  best_loss: 5688.205078, l_rate: 0.000010, lr_count: 0\n",
      "* 88 * loss: 5688.182617,  best_loss: 5688.182617, l_rate: 0.000010, lr_count: 0\n",
      "* 89 * loss: 5688.195312,  best_loss: 5688.182617, l_rate: 0.000010, lr_count: 1\n",
      "* 90 * loss: 5688.191406,  best_loss: 5688.182617, l_rate: 0.000010, lr_count: 2\n",
      "* 91 * loss: 5688.186035,  best_loss: 5688.182617, l_rate: 0.000010, lr_count: 3\n",
      "* 92 * loss: 5688.197754,  best_loss: 5688.182617, l_rate: 0.000010, lr_count: 4\n",
      "* 93 * loss: 5688.181152,  best_loss: 5688.181152, l_rate: 0.000010, lr_count: 0\n",
      "* 94 * loss: 5688.185059,  best_loss: 5688.181152, l_rate: 0.000010, lr_count: 1\n",
      "* 95 * loss: 5688.190430,  best_loss: 5688.181152, l_rate: 0.000010, lr_count: 2\n",
      "* 96 * loss: 5688.183594,  best_loss: 5688.181152, l_rate: 0.000010, lr_count: 3\n",
      "* 97 * loss: 5688.175781,  best_loss: 5688.175781, l_rate: 0.000010, lr_count: 0\n",
      "* 98 * loss: 5688.187500,  best_loss: 5688.175781, l_rate: 0.000010, lr_count: 1\n",
      "* 99 * loss: 5688.184570,  best_loss: 5688.175781, l_rate: 0.000010, lr_count: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 100 * loss: 5688.169434,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 101 * loss: 5688.187500,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 102 * loss: 5688.186523,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 103 * loss: 5688.184082,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 104 * loss: 5688.187500,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 105 * loss: 5688.181152,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 106 * loss: 5688.185547,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 107 * loss: 5688.184082,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 108 * loss: 5688.190918,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 109 * loss: 5688.180176,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 110 * loss: 5688.190430,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 111 * loss: 5688.182617,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 112 * loss: 5688.176270,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 113 * loss: 5688.174805,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 114 * loss: 5688.176758,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 115 * loss: 5688.180664,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 116 * loss: 5688.171875,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 117 * loss: 5688.184082,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 118 * loss: 5688.192871,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 119 * loss: 5688.178223,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 120 * loss: 5688.179688,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 121 * loss: 5688.174316,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 122 * loss: 5688.177246,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 123 * loss: 5688.179199,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 124 * loss: 5688.173828,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 125 * loss: 5688.175781,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 126 * loss: 5688.177734,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 127 * loss: 5688.177246,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 128 * loss: 5688.177246,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 129 * loss: 5688.181152,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 130 * loss: 5688.172363,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 131 * loss: 5688.179199,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 1\n",
      "* 132 * loss: 5688.183105,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 2\n",
      "* 133 * loss: 5688.173340,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 3\n",
      "* 134 * loss: 5688.181641,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 135 * loss: 5688.178223,  best_loss: 5688.169434, l_rate: 0.000010, lr_count: 0\n",
      "* 136 * loss: 5688.165039,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 0\n",
      "* 137 * loss: 5688.177734,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 1\n",
      "* 138 * loss: 5688.171387,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 2\n",
      "* 139 * loss: 5688.182617,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 3\n",
      "* 140 * loss: 5688.166992,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 141 * loss: 5688.176758,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 0\n",
      "* 142 * loss: 5688.169922,  best_loss: 5688.165039, l_rate: 0.000010, lr_count: 1\n",
      "* 143 * loss: 5688.159180,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 0\n",
      "* 144 * loss: 5688.174316,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 1\n",
      "* 145 * loss: 5688.172363,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 2\n",
      "* 146 * loss: 5688.174805,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 3\n",
      "* 147 * loss: 5688.171387,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 148 * loss: 5688.166016,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 0\n",
      "* 149 * loss: 5688.174316,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 1\n",
      "* 150 * loss: 5688.164062,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 2\n",
      "* 151 * loss: 5688.179688,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 3\n",
      "* 152 * loss: 5688.163086,  best_loss: 5688.159180, l_rate: 0.000010, lr_count: 4\n",
      "* 153 * loss: 5688.152832,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 154 * loss: 5688.166504,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 155 * loss: 5688.172852,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 156 * loss: 5688.163574,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 157 * loss: 5688.177246,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 158 * loss: 5688.158203,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 159 * loss: 5688.156738,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 160 * loss: 5688.178711,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 161 * loss: 5688.173828,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 162 * loss: 5688.164062,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 163 * loss: 5688.159180,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 164 * loss: 5688.161133,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 165 * loss: 5688.169922,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 166 * loss: 5688.166504,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 167 * loss: 5688.172852,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 168 * loss: 5688.153809,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 169 * loss: 5688.164062,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 170 * loss: 5688.166016,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 171 * loss: 5688.160645,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 172 * loss: 5688.163574,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 173 * loss: 5688.155762,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 174 * loss: 5688.167969,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 175 * loss: 5688.163086,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 176 * loss: 5688.167480,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 177 * loss: 5688.164551,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 178 * loss: 5688.167969,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 179 * loss: 5688.156250,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 180 * loss: 5688.176758,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 181 * loss: 5688.162598,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 182 * loss: 5688.160156,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 183 * loss: 5688.162598,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 184 * loss: 5688.162109,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 185 * loss: 5688.165039,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 186 * loss: 5688.157227,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 187 * loss: 5688.163574,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 188 * loss: 5688.159180,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 0\n",
      "* 189 * loss: 5688.161133,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 1\n",
      "* 190 * loss: 5688.167969,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 2\n",
      "* 191 * loss: 5688.166992,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 3\n",
      "* 192 * loss: 5688.173340,  best_loss: 5688.152832, l_rate: 0.000010, lr_count: 4\n",
      "* 193 * loss: 5688.151367,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 0\n",
      "* 194 * loss: 5688.175293,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 195 * loss: 5688.155273,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 2\n",
      "* 196 * loss: 5688.171387,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 3\n",
      "* 197 * loss: 5688.166504,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 198 * loss: 5688.163574,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 0\n",
      "* 199 * loss: 5688.160156,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 1\n",
      "* 200 * loss: 5688.165039,  best_loss: 5688.151367, l_rate: 0.000010, lr_count: 2\n",
      "* 201 * loss: 5688.142578,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 202 * loss: 5688.159668,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 203 * loss: 5688.170898,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 204 * loss: 5688.154297,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 205 * loss: 5688.159668,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 206 * loss: 5688.153320,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 207 * loss: 5688.165527,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 208 * loss: 5688.163574,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 209 * loss: 5688.155762,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 210 * loss: 5688.160156,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 211 * loss: 5688.162598,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 212 * loss: 5688.150391,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 213 * loss: 5688.158203,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 214 * loss: 5688.161133,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 215 * loss: 5688.153320,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 216 * loss: 5688.156738,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 217 * loss: 5688.154297,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 218 * loss: 5688.146973,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 219 * loss: 5688.151855,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 220 * loss: 5688.157715,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 221 * loss: 5688.156250,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 222 * loss: 5688.153320,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 223 * loss: 5688.162109,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 224 * loss: 5688.157227,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 225 * loss: 5688.158203,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 226 * loss: 5688.153809,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 227 * loss: 5688.163086,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 228 * loss: 5688.156738,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 229 * loss: 5688.150391,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 230 * loss: 5688.152344,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 231 * loss: 5688.160156,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 232 * loss: 5688.147461,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n",
      "* 233 * loss: 5688.156250,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 2\n",
      "* 234 * loss: 5688.148438,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 3\n",
      "* 235 * loss: 5688.160645,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 236 * loss: 5688.152832,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 0\n",
      "* 237 * loss: 5688.149414,  best_loss: 5688.142578, l_rate: 0.000010, lr_count: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-08f8d64d0f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get loss and leraning rate at this epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "# Initialize values\n",
    "best_loss, count = float('inf'), 0\n",
    "\n",
    "# Start epoch loop\n",
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in train_ds:\n",
    "        train_step(inputs)\n",
    "    \n",
    "    # Get loss and leraning rate at this epoch\n",
    "    t_loss = train_loss.result().numpy() \n",
    "    l_rate = optimizer.learning_rate.numpy()\n",
    "\n",
    "    # Control learning rate\n",
    "    count, lr  = reduce_lr(best_loss, t_loss, count, l_rate, 5, 0.2, 0.00001)\n",
    "    optimizer.learning_rate = lr\n",
    "    \n",
    "    # Save checkpoint if best v_loss \n",
    "    if t_loss < best_loss:\n",
    "        best_loss = t_loss\n",
    "        checkpoint.save(file_prefix=os.path.join(save_dir+'/ckp/', 'ckp'))\n",
    "    \n",
    "    # Save loss, lerning rate\n",
    "    print(\"* %i * loss: %f,  best_loss: %f, l_rate: %f, lr_count: %i\"%(epoch, t_loss, best_loss, l_rate, count ))\n",
    "    df = pd.DataFrame({'epoch':[epoch], 'loss':[t_loss], 'best_loss':[best_loss], 'l_rate':[l_rate]  } )\n",
    "    df.to_csv(save_dir+'/process.csv', mode='a', header=False)\n",
    "    \n",
    "    # Reset loss\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
