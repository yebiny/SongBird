{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from models import *\n",
    "from def_dict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51721, 128, 64, 1), (51721, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data/w58y67/preprocessing_2/step_2/'\n",
    "\n",
    "data = np.load(data_path+'data_same.npy')\n",
    "data = np.reshape(data, (data.shape[0], data.shape[1], data.shape[2], 1))\n",
    "data_info = np.load(data_path+'data_info.npy')\n",
    "data.shape, data_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'train_result/train_5/'\n",
    "if_not_make(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Training data shape:  (46548, 128, 64, 1)\n",
      "* Test data shape :  (5173, 128, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, data_info, test_size=0.1, random_state=321)\n",
    "print('* Training data shape: ', x_train.shape)\n",
    "#print('* Validataion data shape : ', x_valid.shape)\n",
    "print('* Test data shape : ', x_test.shape)\n",
    "\n",
    "np.save(save_path+'x_train', x_train)\n",
    "np.save(save_path+'y_train', y_train)\n",
    "np.save(save_path+'x_test', x_test)\n",
    "np.save(save_path+'y_test', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 128, 64, 1), (None, 128, 64, 1)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train/255\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, x_train)).batch(256)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, vae = build_vae(x_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 128, 64, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 32, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 32, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 16, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 16, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 8, 32)    9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 8, 32)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 4, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 4, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1024)         0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          262400      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           2570        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           2570        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 361,396\n",
      "Trainable params: 361,396\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 64, 1)]      0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  361396    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 128, 64, 1)        359809    \n",
      "=================================================================\n",
      "Total params: 721,205\n",
      "Trainable params: 721,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7f9d6f58b590>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckp_dir = save_path+'/ckp/'\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder, decoder=decoder, vae=vae)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(ckp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "\n",
    "def get_rec_loss(inputs, predictions):\n",
    "    rec_loss = tf.keras.losses.binary_crossentropy(inputs, predictions)\n",
    "    rec_loss = tf.reduce_mean(rec_loss)\n",
    "    rec_loss *= x_train.shape[1]*x_train.shape[2]\n",
    "    return rec_loss\n",
    "\n",
    "def get_kl_loss(z_log_var, z_mean):\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    kl_loss *= -0.5\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Get model ouputs\n",
    "        z_log_var, z_mean, z = encoder(inputs)\n",
    "        predictions = decoder(z)\n",
    "        \n",
    "        # Compute losses\n",
    "        rec_loss = get_rec_loss(inputs, predictions)\n",
    "        kl_loss = get_kl_loss(z_log_var, z_mean)\n",
    "        loss = rec_loss + kl_loss\n",
    "    \n",
    "    # Compute gradients\n",
    "    varialbes = vae.trainable_variables\n",
    "    gradients = tape.gradient(loss, varialbes)\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(gradients, varialbes))\n",
    "    \n",
    "    # Update train loss\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lr(pre_v_loss, v_loss, count, lr, patience, factor, min_lr):\n",
    "    if v_loss < pre_v_loss:\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        if count >= patience: \n",
    "            lr = lr*factor\n",
    "            if lr < min_lr: \n",
    "                lr = min_lr\n",
    "            count = 0\n",
    "            print('reduce learning rate..', lr)    \n",
    "    return count, lr\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(1), encoder=encoder, decoder=decoder, vae=vae)\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(save_path+'/training.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 * loss: 5382.562012,  best_loss: 5382.562012, l_rate: 0.001000, lr_count: 0\n",
      "* 1 * loss: 5283.356934,  best_loss: 5283.356934, l_rate: 0.001000, lr_count: 0\n",
      "* 2 * loss: 5254.441895,  best_loss: 5254.441895, l_rate: 0.001000, lr_count: 0\n",
      "* 3 * loss: 5234.719727,  best_loss: 5234.719727, l_rate: 0.001000, lr_count: 0\n",
      "* 4 * loss: 5225.055176,  best_loss: 5225.055176, l_rate: 0.001000, lr_count: 0\n",
      "* 5 * loss: 5219.903320,  best_loss: 5219.903320, l_rate: 0.001000, lr_count: 0\n",
      "* 6 * loss: 5216.562988,  best_loss: 5216.562988, l_rate: 0.001000, lr_count: 0\n",
      "* 7 * loss: 5214.093750,  best_loss: 5214.093750, l_rate: 0.001000, lr_count: 0\n",
      "* 8 * loss: 5212.419434,  best_loss: 5212.419434, l_rate: 0.001000, lr_count: 0\n",
      "* 9 * loss: 5211.175781,  best_loss: 5211.175781, l_rate: 0.001000, lr_count: 0\n",
      "* 10 * loss: 5209.964844,  best_loss: 5209.964844, l_rate: 0.001000, lr_count: 0\n",
      "* 11 * loss: 5208.466309,  best_loss: 5208.466309, l_rate: 0.001000, lr_count: 0\n",
      "* 12 * loss: 5207.567383,  best_loss: 5207.567383, l_rate: 0.001000, lr_count: 0\n",
      "* 13 * loss: 5206.520996,  best_loss: 5206.520996, l_rate: 0.001000, lr_count: 0\n",
      "* 14 * loss: 5205.862305,  best_loss: 5205.862305, l_rate: 0.001000, lr_count: 0\n",
      "* 15 * loss: 5205.239746,  best_loss: 5205.239746, l_rate: 0.001000, lr_count: 0\n",
      "* 16 * loss: 5204.350586,  best_loss: 5204.350586, l_rate: 0.001000, lr_count: 0\n",
      "* 17 * loss: 5203.991211,  best_loss: 5203.991211, l_rate: 0.001000, lr_count: 0\n",
      "* 18 * loss: 5203.497070,  best_loss: 5203.497070, l_rate: 0.001000, lr_count: 0\n",
      "* 19 * loss: 5202.825684,  best_loss: 5202.825684, l_rate: 0.001000, lr_count: 0\n",
      "* 20 * loss: 5202.439941,  best_loss: 5202.439941, l_rate: 0.001000, lr_count: 0\n",
      "* 21 * loss: 5202.094727,  best_loss: 5202.094727, l_rate: 0.001000, lr_count: 0\n",
      "* 22 * loss: 5201.875977,  best_loss: 5201.875977, l_rate: 0.001000, lr_count: 0\n",
      "* 23 * loss: 5201.483398,  best_loss: 5201.483398, l_rate: 0.001000, lr_count: 0\n",
      "* 24 * loss: 5201.132324,  best_loss: 5201.132324, l_rate: 0.001000, lr_count: 0\n",
      "* 25 * loss: 5201.074707,  best_loss: 5201.074707, l_rate: 0.001000, lr_count: 0\n",
      "* 26 * loss: 5200.540527,  best_loss: 5200.540527, l_rate: 0.001000, lr_count: 0\n",
      "* 27 * loss: 5200.542969,  best_loss: 5200.540527, l_rate: 0.001000, lr_count: 1\n",
      "* 28 * loss: 5200.258789,  best_loss: 5200.258789, l_rate: 0.001000, lr_count: 0\n",
      "* 29 * loss: 5200.234375,  best_loss: 5200.234375, l_rate: 0.001000, lr_count: 0\n",
      "* 30 * loss: 5199.801270,  best_loss: 5199.801270, l_rate: 0.001000, lr_count: 0\n",
      "* 31 * loss: 5199.799805,  best_loss: 5199.799805, l_rate: 0.001000, lr_count: 0\n",
      "* 32 * loss: 5199.545898,  best_loss: 5199.545898, l_rate: 0.001000, lr_count: 0\n",
      "* 33 * loss: 5199.522461,  best_loss: 5199.522461, l_rate: 0.001000, lr_count: 0\n",
      "* 34 * loss: 5199.123047,  best_loss: 5199.123047, l_rate: 0.001000, lr_count: 0\n",
      "* 35 * loss: 5199.132812,  best_loss: 5199.123047, l_rate: 0.001000, lr_count: 1\n",
      "* 36 * loss: 5198.993652,  best_loss: 5198.993652, l_rate: 0.001000, lr_count: 0\n",
      "* 37 * loss: 5198.745605,  best_loss: 5198.745605, l_rate: 0.001000, lr_count: 0\n",
      "* 38 * loss: 5198.752930,  best_loss: 5198.745605, l_rate: 0.001000, lr_count: 1\n",
      "* 39 * loss: 5198.510742,  best_loss: 5198.510742, l_rate: 0.001000, lr_count: 0\n",
      "* 40 * loss: 5198.431641,  best_loss: 5198.431641, l_rate: 0.001000, lr_count: 0\n",
      "* 41 * loss: 5198.332520,  best_loss: 5198.332520, l_rate: 0.001000, lr_count: 0\n",
      "* 42 * loss: 5198.208984,  best_loss: 5198.208984, l_rate: 0.001000, lr_count: 0\n",
      "* 43 * loss: 5198.217773,  best_loss: 5198.208984, l_rate: 0.001000, lr_count: 1\n",
      "* 44 * loss: 5198.143066,  best_loss: 5198.143066, l_rate: 0.001000, lr_count: 0\n",
      "* 45 * loss: 5197.829102,  best_loss: 5197.829102, l_rate: 0.001000, lr_count: 0\n",
      "* 46 * loss: 5197.718262,  best_loss: 5197.718262, l_rate: 0.001000, lr_count: 0\n",
      "* 47 * loss: 5197.626465,  best_loss: 5197.626465, l_rate: 0.001000, lr_count: 0\n",
      "* 48 * loss: 5197.581543,  best_loss: 5197.581543, l_rate: 0.001000, lr_count: 0\n",
      "* 49 * loss: 5197.610840,  best_loss: 5197.581543, l_rate: 0.001000, lr_count: 1\n",
      "* 50 * loss: 5197.291504,  best_loss: 5197.291504, l_rate: 0.001000, lr_count: 0\n",
      "* 51 * loss: 5197.344238,  best_loss: 5197.291504, l_rate: 0.001000, lr_count: 1\n",
      "* 52 * loss: 5197.282227,  best_loss: 5197.282227, l_rate: 0.001000, lr_count: 0\n",
      "* 53 * loss: 5197.117188,  best_loss: 5197.117188, l_rate: 0.001000, lr_count: 0\n",
      "* 54 * loss: 5197.187988,  best_loss: 5197.117188, l_rate: 0.001000, lr_count: 1\n",
      "* 55 * loss: 5197.061523,  best_loss: 5197.061523, l_rate: 0.001000, lr_count: 0\n",
      "* 56 * loss: 5197.018066,  best_loss: 5197.018066, l_rate: 0.001000, lr_count: 0\n",
      "* 57 * loss: 5196.875000,  best_loss: 5196.875000, l_rate: 0.001000, lr_count: 0\n",
      "* 58 * loss: 5196.903320,  best_loss: 5196.875000, l_rate: 0.001000, lr_count: 1\n",
      "* 59 * loss: 5196.718262,  best_loss: 5196.718262, l_rate: 0.001000, lr_count: 0\n",
      "* 60 * loss: 5196.668457,  best_loss: 5196.668457, l_rate: 0.001000, lr_count: 0\n",
      "* 61 * loss: 5196.451660,  best_loss: 5196.451660, l_rate: 0.001000, lr_count: 0\n",
      "* 62 * loss: 5196.466309,  best_loss: 5196.451660, l_rate: 0.001000, lr_count: 1\n",
      "* 63 * loss: 5196.509766,  best_loss: 5196.451660, l_rate: 0.001000, lr_count: 2\n",
      "* 64 * loss: 5196.487793,  best_loss: 5196.451660, l_rate: 0.001000, lr_count: 3\n",
      "* 65 * loss: 5196.424316,  best_loss: 5196.424316, l_rate: 0.001000, lr_count: 0\n",
      "* 66 * loss: 5196.258789,  best_loss: 5196.258789, l_rate: 0.001000, lr_count: 0\n",
      "* 67 * loss: 5196.273926,  best_loss: 5196.258789, l_rate: 0.001000, lr_count: 1\n",
      "* 68 * loss: 5196.349121,  best_loss: 5196.258789, l_rate: 0.001000, lr_count: 2\n",
      "* 69 * loss: 5196.164062,  best_loss: 5196.164062, l_rate: 0.001000, lr_count: 0\n",
      "* 70 * loss: 5196.160156,  best_loss: 5196.160156, l_rate: 0.001000, lr_count: 0\n",
      "* 71 * loss: 5196.070801,  best_loss: 5196.070801, l_rate: 0.001000, lr_count: 0\n",
      "* 72 * loss: 5195.986328,  best_loss: 5195.986328, l_rate: 0.001000, lr_count: 0\n",
      "* 73 * loss: 5195.828125,  best_loss: 5195.828125, l_rate: 0.001000, lr_count: 0\n",
      "* 74 * loss: 5195.875977,  best_loss: 5195.828125, l_rate: 0.001000, lr_count: 1\n",
      "* 75 * loss: 5195.770508,  best_loss: 5195.770508, l_rate: 0.001000, lr_count: 0\n",
      "* 76 * loss: 5195.875488,  best_loss: 5195.770508, l_rate: 0.001000, lr_count: 1\n",
      "* 77 * loss: 5195.898438,  best_loss: 5195.770508, l_rate: 0.001000, lr_count: 2\n",
      "* 78 * loss: 5195.644043,  best_loss: 5195.644043, l_rate: 0.001000, lr_count: 0\n",
      "* 79 * loss: 5195.609375,  best_loss: 5195.609375, l_rate: 0.001000, lr_count: 0\n",
      "* 80 * loss: 5195.591797,  best_loss: 5195.591797, l_rate: 0.001000, lr_count: 0\n",
      "* 81 * loss: 5195.575684,  best_loss: 5195.575684, l_rate: 0.001000, lr_count: 0\n",
      "* 82 * loss: 5195.604004,  best_loss: 5195.575684, l_rate: 0.001000, lr_count: 1\n",
      "* 83 * loss: 5195.538574,  best_loss: 5195.538574, l_rate: 0.001000, lr_count: 0\n",
      "* 84 * loss: 5195.416504,  best_loss: 5195.416504, l_rate: 0.001000, lr_count: 0\n",
      "* 85 * loss: 5195.410156,  best_loss: 5195.410156, l_rate: 0.001000, lr_count: 0\n",
      "* 86 * loss: 5195.408203,  best_loss: 5195.408203, l_rate: 0.001000, lr_count: 0\n",
      "* 87 * loss: 5195.495117,  best_loss: 5195.408203, l_rate: 0.001000, lr_count: 1\n",
      "* 88 * loss: 5195.301758,  best_loss: 5195.301758, l_rate: 0.001000, lr_count: 0\n",
      "* 89 * loss: 5195.187500,  best_loss: 5195.187500, l_rate: 0.001000, lr_count: 0\n",
      "* 90 * loss: 5195.154785,  best_loss: 5195.154785, l_rate: 0.001000, lr_count: 0\n",
      "* 91 * loss: 5195.238281,  best_loss: 5195.154785, l_rate: 0.001000, lr_count: 1\n",
      "* 92 * loss: 5195.212402,  best_loss: 5195.154785, l_rate: 0.001000, lr_count: 2\n",
      "* 93 * loss: 5195.274414,  best_loss: 5195.154785, l_rate: 0.001000, lr_count: 3\n",
      "* 94 * loss: 5195.040527,  best_loss: 5195.040527, l_rate: 0.001000, lr_count: 0\n",
      "* 95 * loss: 5195.072266,  best_loss: 5195.040527, l_rate: 0.001000, lr_count: 1\n",
      "* 96 * loss: 5195.129883,  best_loss: 5195.040527, l_rate: 0.001000, lr_count: 2\n",
      "* 97 * loss: 5195.136719,  best_loss: 5195.040527, l_rate: 0.001000, lr_count: 3\n",
      "* 98 * loss: 5194.888184,  best_loss: 5194.888184, l_rate: 0.001000, lr_count: 0\n",
      "* 99 * loss: 5194.937012,  best_loss: 5194.888184, l_rate: 0.001000, lr_count: 1\n",
      "* 100 * loss: 5194.893555,  best_loss: 5194.888184, l_rate: 0.001000, lr_count: 2\n",
      "* 101 * loss: 5194.882324,  best_loss: 5194.882324, l_rate: 0.001000, lr_count: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 102 * loss: 5194.838379,  best_loss: 5194.838379, l_rate: 0.001000, lr_count: 0\n",
      "* 103 * loss: 5194.937012,  best_loss: 5194.838379, l_rate: 0.001000, lr_count: 1\n",
      "* 104 * loss: 5194.965332,  best_loss: 5194.838379, l_rate: 0.001000, lr_count: 2\n",
      "* 105 * loss: 5194.795410,  best_loss: 5194.795410, l_rate: 0.001000, lr_count: 0\n",
      "* 106 * loss: 5194.719727,  best_loss: 5194.719727, l_rate: 0.001000, lr_count: 0\n",
      "* 107 * loss: 5194.660156,  best_loss: 5194.660156, l_rate: 0.001000, lr_count: 0\n",
      "* 108 * loss: 5194.770020,  best_loss: 5194.660156, l_rate: 0.001000, lr_count: 1\n",
      "* 109 * loss: 5194.568848,  best_loss: 5194.568848, l_rate: 0.001000, lr_count: 0\n",
      "* 110 * loss: 5194.729980,  best_loss: 5194.568848, l_rate: 0.001000, lr_count: 1\n",
      "* 111 * loss: 5194.738281,  best_loss: 5194.568848, l_rate: 0.001000, lr_count: 2\n",
      "* 112 * loss: 5194.744141,  best_loss: 5194.568848, l_rate: 0.001000, lr_count: 3\n",
      "* 113 * loss: 5194.624512,  best_loss: 5194.568848, l_rate: 0.001000, lr_count: 4\n",
      "* 114 * loss: 5194.483398,  best_loss: 5194.483398, l_rate: 0.001000, lr_count: 0\n",
      "* 115 * loss: 5194.475098,  best_loss: 5194.475098, l_rate: 0.001000, lr_count: 0\n",
      "* 116 * loss: 5194.447266,  best_loss: 5194.447266, l_rate: 0.001000, lr_count: 0\n",
      "* 117 * loss: 5194.603516,  best_loss: 5194.447266, l_rate: 0.001000, lr_count: 1\n",
      "* 118 * loss: 5194.635742,  best_loss: 5194.447266, l_rate: 0.001000, lr_count: 2\n",
      "* 119 * loss: 5194.482422,  best_loss: 5194.447266, l_rate: 0.001000, lr_count: 3\n",
      "* 120 * loss: 5194.342773,  best_loss: 5194.342773, l_rate: 0.001000, lr_count: 0\n",
      "* 121 * loss: 5194.330566,  best_loss: 5194.330566, l_rate: 0.001000, lr_count: 0\n",
      "* 122 * loss: 5194.369629,  best_loss: 5194.330566, l_rate: 0.001000, lr_count: 1\n",
      "* 123 * loss: 5194.422852,  best_loss: 5194.330566, l_rate: 0.001000, lr_count: 2\n",
      "* 124 * loss: 5194.424805,  best_loss: 5194.330566, l_rate: 0.001000, lr_count: 3\n",
      "* 125 * loss: 5194.380371,  best_loss: 5194.330566, l_rate: 0.001000, lr_count: 4\n",
      "* 126 * loss: 5194.194824,  best_loss: 5194.194824, l_rate: 0.001000, lr_count: 0\n",
      "* 127 * loss: 5194.259766,  best_loss: 5194.194824, l_rate: 0.001000, lr_count: 1\n",
      "* 128 * loss: 5194.250000,  best_loss: 5194.194824, l_rate: 0.001000, lr_count: 2\n",
      "* 129 * loss: 5194.177734,  best_loss: 5194.177734, l_rate: 0.001000, lr_count: 0\n",
      "* 130 * loss: 5194.246582,  best_loss: 5194.177734, l_rate: 0.001000, lr_count: 1\n",
      "* 131 * loss: 5194.312012,  best_loss: 5194.177734, l_rate: 0.001000, lr_count: 2\n",
      "* 132 * loss: 5194.193848,  best_loss: 5194.177734, l_rate: 0.001000, lr_count: 3\n",
      "* 133 * loss: 5194.207520,  best_loss: 5194.177734, l_rate: 0.001000, lr_count: 4\n",
      "reduce learning rate.. 0.00020000000949949026\n",
      "* 134 * loss: 5194.281250,  best_loss: 5194.177734, l_rate: 0.001000, lr_count: 0\n",
      "* 135 * loss: 5193.494141,  best_loss: 5193.494141, l_rate: 0.000200, lr_count: 0\n",
      "* 136 * loss: 5193.414551,  best_loss: 5193.414551, l_rate: 0.000200, lr_count: 0\n",
      "* 137 * loss: 5193.408203,  best_loss: 5193.408203, l_rate: 0.000200, lr_count: 0\n",
      "* 138 * loss: 5193.403320,  best_loss: 5193.403320, l_rate: 0.000200, lr_count: 0\n",
      "* 139 * loss: 5193.395020,  best_loss: 5193.395020, l_rate: 0.000200, lr_count: 0\n",
      "* 140 * loss: 5193.397461,  best_loss: 5193.395020, l_rate: 0.000200, lr_count: 1\n",
      "* 141 * loss: 5193.390137,  best_loss: 5193.390137, l_rate: 0.000200, lr_count: 0\n",
      "* 142 * loss: 5193.381836,  best_loss: 5193.381836, l_rate: 0.000200, lr_count: 0\n",
      "* 143 * loss: 5193.371094,  best_loss: 5193.371094, l_rate: 0.000200, lr_count: 0\n",
      "* 144 * loss: 5193.399902,  best_loss: 5193.371094, l_rate: 0.000200, lr_count: 1\n",
      "* 145 * loss: 5193.375000,  best_loss: 5193.371094, l_rate: 0.000200, lr_count: 2\n",
      "* 146 * loss: 5193.381836,  best_loss: 5193.371094, l_rate: 0.000200, lr_count: 3\n",
      "* 147 * loss: 5193.368652,  best_loss: 5193.368652, l_rate: 0.000200, lr_count: 0\n",
      "* 148 * loss: 5193.374023,  best_loss: 5193.368652, l_rate: 0.000200, lr_count: 1\n",
      "* 149 * loss: 5193.354980,  best_loss: 5193.354980, l_rate: 0.000200, lr_count: 0\n",
      "* 150 * loss: 5193.340820,  best_loss: 5193.340820, l_rate: 0.000200, lr_count: 0\n",
      "* 151 * loss: 5193.345215,  best_loss: 5193.340820, l_rate: 0.000200, lr_count: 1\n",
      "* 152 * loss: 5193.341797,  best_loss: 5193.340820, l_rate: 0.000200, lr_count: 2\n",
      "* 153 * loss: 5193.326172,  best_loss: 5193.326172, l_rate: 0.000200, lr_count: 0\n",
      "* 154 * loss: 5193.320801,  best_loss: 5193.320801, l_rate: 0.000200, lr_count: 0\n",
      "* 155 * loss: 5193.312988,  best_loss: 5193.312988, l_rate: 0.000200, lr_count: 0\n",
      "* 156 * loss: 5193.314941,  best_loss: 5193.312988, l_rate: 0.000200, lr_count: 1\n",
      "* 157 * loss: 5193.305176,  best_loss: 5193.305176, l_rate: 0.000200, lr_count: 0\n",
      "* 158 * loss: 5193.301270,  best_loss: 5193.301270, l_rate: 0.000200, lr_count: 0\n",
      "* 159 * loss: 5193.305176,  best_loss: 5193.301270, l_rate: 0.000200, lr_count: 1\n",
      "* 160 * loss: 5193.288574,  best_loss: 5193.288574, l_rate: 0.000200, lr_count: 0\n",
      "* 161 * loss: 5193.284668,  best_loss: 5193.284668, l_rate: 0.000200, lr_count: 0\n",
      "* 162 * loss: 5193.271484,  best_loss: 5193.271484, l_rate: 0.000200, lr_count: 0\n",
      "* 163 * loss: 5193.244141,  best_loss: 5193.244141, l_rate: 0.000200, lr_count: 0\n",
      "* 164 * loss: 5193.275391,  best_loss: 5193.244141, l_rate: 0.000200, lr_count: 1\n",
      "* 165 * loss: 5193.247559,  best_loss: 5193.244141, l_rate: 0.000200, lr_count: 2\n",
      "* 166 * loss: 5193.241699,  best_loss: 5193.241699, l_rate: 0.000200, lr_count: 0\n",
      "* 167 * loss: 5193.239258,  best_loss: 5193.239258, l_rate: 0.000200, lr_count: 0\n",
      "* 168 * loss: 5193.231445,  best_loss: 5193.231445, l_rate: 0.000200, lr_count: 0\n",
      "* 169 * loss: 5193.236328,  best_loss: 5193.231445, l_rate: 0.000200, lr_count: 1\n",
      "* 170 * loss: 5193.217773,  best_loss: 5193.217773, l_rate: 0.000200, lr_count: 0\n",
      "* 171 * loss: 5193.227539,  best_loss: 5193.217773, l_rate: 0.000200, lr_count: 1\n",
      "* 172 * loss: 5193.212402,  best_loss: 5193.212402, l_rate: 0.000200, lr_count: 0\n",
      "* 173 * loss: 5193.205078,  best_loss: 5193.205078, l_rate: 0.000200, lr_count: 0\n",
      "* 174 * loss: 5193.205078,  best_loss: 5193.205078, l_rate: 0.000200, lr_count: 1\n",
      "* 175 * loss: 5193.213867,  best_loss: 5193.205078, l_rate: 0.000200, lr_count: 2\n",
      "* 176 * loss: 5193.177734,  best_loss: 5193.177734, l_rate: 0.000200, lr_count: 0\n",
      "* 177 * loss: 5193.172852,  best_loss: 5193.172852, l_rate: 0.000200, lr_count: 0\n",
      "* 178 * loss: 5193.166016,  best_loss: 5193.166016, l_rate: 0.000200, lr_count: 0\n",
      "* 179 * loss: 5193.166504,  best_loss: 5193.166016, l_rate: 0.000200, lr_count: 1\n",
      "* 180 * loss: 5193.155273,  best_loss: 5193.155273, l_rate: 0.000200, lr_count: 0\n",
      "* 181 * loss: 5193.139648,  best_loss: 5193.139648, l_rate: 0.000200, lr_count: 0\n",
      "* 182 * loss: 5193.158203,  best_loss: 5193.139648, l_rate: 0.000200, lr_count: 1\n",
      "* 183 * loss: 5193.147461,  best_loss: 5193.139648, l_rate: 0.000200, lr_count: 2\n",
      "* 184 * loss: 5193.143555,  best_loss: 5193.139648, l_rate: 0.000200, lr_count: 3\n",
      "* 185 * loss: 5193.130371,  best_loss: 5193.130371, l_rate: 0.000200, lr_count: 0\n",
      "* 186 * loss: 5193.148926,  best_loss: 5193.130371, l_rate: 0.000200, lr_count: 1\n",
      "* 187 * loss: 5193.122070,  best_loss: 5193.122070, l_rate: 0.000200, lr_count: 0\n",
      "* 188 * loss: 5193.137695,  best_loss: 5193.122070, l_rate: 0.000200, lr_count: 1\n",
      "* 189 * loss: 5193.111816,  best_loss: 5193.111816, l_rate: 0.000200, lr_count: 0\n",
      "* 190 * loss: 5193.109375,  best_loss: 5193.109375, l_rate: 0.000200, lr_count: 0\n",
      "* 191 * loss: 5193.111328,  best_loss: 5193.109375, l_rate: 0.000200, lr_count: 1\n",
      "* 192 * loss: 5193.116699,  best_loss: 5193.109375, l_rate: 0.000200, lr_count: 2\n",
      "* 193 * loss: 5193.102539,  best_loss: 5193.102539, l_rate: 0.000200, lr_count: 0\n",
      "* 194 * loss: 5193.071777,  best_loss: 5193.071777, l_rate: 0.000200, lr_count: 0\n",
      "* 195 * loss: 5193.063965,  best_loss: 5193.063965, l_rate: 0.000200, lr_count: 0\n",
      "* 196 * loss: 5193.081543,  best_loss: 5193.063965, l_rate: 0.000200, lr_count: 1\n",
      "* 197 * loss: 5193.085449,  best_loss: 5193.063965, l_rate: 0.000200, lr_count: 2\n",
      "* 198 * loss: 5193.062988,  best_loss: 5193.062988, l_rate: 0.000200, lr_count: 0\n",
      "* 199 * loss: 5193.078613,  best_loss: 5193.062988, l_rate: 0.000200, lr_count: 1\n",
      "* 200 * loss: 5193.079102,  best_loss: 5193.062988, l_rate: 0.000200, lr_count: 2\n",
      "* 201 * loss: 5193.064941,  best_loss: 5193.062988, l_rate: 0.000200, lr_count: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 202 * loss: 5193.037109,  best_loss: 5193.037109, l_rate: 0.000200, lr_count: 0\n",
      "* 203 * loss: 5193.040039,  best_loss: 5193.037109, l_rate: 0.000200, lr_count: 1\n",
      "* 204 * loss: 5193.035645,  best_loss: 5193.035645, l_rate: 0.000200, lr_count: 0\n",
      "* 205 * loss: 5193.057129,  best_loss: 5193.035645, l_rate: 0.000200, lr_count: 1\n",
      "* 206 * loss: 5193.034180,  best_loss: 5193.034180, l_rate: 0.000200, lr_count: 0\n",
      "* 207 * loss: 5193.027344,  best_loss: 5193.027344, l_rate: 0.000200, lr_count: 0\n",
      "* 208 * loss: 5193.014648,  best_loss: 5193.014648, l_rate: 0.000200, lr_count: 0\n",
      "* 209 * loss: 5193.028320,  best_loss: 5193.014648, l_rate: 0.000200, lr_count: 1\n",
      "* 210 * loss: 5193.005859,  best_loss: 5193.005859, l_rate: 0.000200, lr_count: 0\n",
      "* 211 * loss: 5192.998535,  best_loss: 5192.998535, l_rate: 0.000200, lr_count: 0\n",
      "* 212 * loss: 5193.026367,  best_loss: 5192.998535, l_rate: 0.000200, lr_count: 1\n",
      "* 213 * loss: 5192.989746,  best_loss: 5192.989746, l_rate: 0.000200, lr_count: 0\n",
      "* 214 * loss: 5192.996094,  best_loss: 5192.989746, l_rate: 0.000200, lr_count: 1\n",
      "* 215 * loss: 5192.977539,  best_loss: 5192.977539, l_rate: 0.000200, lr_count: 0\n",
      "* 216 * loss: 5192.978516,  best_loss: 5192.977539, l_rate: 0.000200, lr_count: 1\n",
      "* 217 * loss: 5192.980957,  best_loss: 5192.977539, l_rate: 0.000200, lr_count: 2\n",
      "* 218 * loss: 5192.979980,  best_loss: 5192.977539, l_rate: 0.000200, lr_count: 3\n",
      "* 219 * loss: 5192.970215,  best_loss: 5192.970215, l_rate: 0.000200, lr_count: 0\n",
      "* 220 * loss: 5192.956055,  best_loss: 5192.956055, l_rate: 0.000200, lr_count: 0\n",
      "* 221 * loss: 5192.959473,  best_loss: 5192.956055, l_rate: 0.000200, lr_count: 1\n",
      "* 222 * loss: 5192.974609,  best_loss: 5192.956055, l_rate: 0.000200, lr_count: 2\n",
      "* 223 * loss: 5192.955566,  best_loss: 5192.955566, l_rate: 0.000200, lr_count: 0\n",
      "* 224 * loss: 5192.941406,  best_loss: 5192.941406, l_rate: 0.000200, lr_count: 0\n",
      "* 225 * loss: 5192.945801,  best_loss: 5192.941406, l_rate: 0.000200, lr_count: 1\n",
      "* 226 * loss: 5192.942383,  best_loss: 5192.941406, l_rate: 0.000200, lr_count: 2\n",
      "* 227 * loss: 5192.930176,  best_loss: 5192.930176, l_rate: 0.000200, lr_count: 0\n",
      "* 228 * loss: 5192.940918,  best_loss: 5192.930176, l_rate: 0.000200, lr_count: 1\n",
      "* 229 * loss: 5192.926270,  best_loss: 5192.926270, l_rate: 0.000200, lr_count: 0\n",
      "* 230 * loss: 5192.933105,  best_loss: 5192.926270, l_rate: 0.000200, lr_count: 1\n",
      "* 231 * loss: 5192.916016,  best_loss: 5192.916016, l_rate: 0.000200, lr_count: 0\n",
      "* 232 * loss: 5192.911621,  best_loss: 5192.911621, l_rate: 0.000200, lr_count: 0\n",
      "* 233 * loss: 5192.913086,  best_loss: 5192.911621, l_rate: 0.000200, lr_count: 1\n",
      "* 234 * loss: 5192.911621,  best_loss: 5192.911621, l_rate: 0.000200, lr_count: 2\n",
      "* 235 * loss: 5192.930664,  best_loss: 5192.911621, l_rate: 0.000200, lr_count: 3\n",
      "* 236 * loss: 5192.903809,  best_loss: 5192.903809, l_rate: 0.000200, lr_count: 0\n",
      "* 237 * loss: 5192.899414,  best_loss: 5192.899414, l_rate: 0.000200, lr_count: 0\n",
      "* 238 * loss: 5192.888184,  best_loss: 5192.888184, l_rate: 0.000200, lr_count: 0\n",
      "* 239 * loss: 5192.912598,  best_loss: 5192.888184, l_rate: 0.000200, lr_count: 1\n",
      "* 240 * loss: 5192.892090,  best_loss: 5192.888184, l_rate: 0.000200, lr_count: 2\n",
      "* 241 * loss: 5192.859375,  best_loss: 5192.859375, l_rate: 0.000200, lr_count: 0\n",
      "* 242 * loss: 5192.893555,  best_loss: 5192.859375, l_rate: 0.000200, lr_count: 1\n",
      "* 243 * loss: 5192.887207,  best_loss: 5192.859375, l_rate: 0.000200, lr_count: 2\n",
      "* 244 * loss: 5192.867676,  best_loss: 5192.859375, l_rate: 0.000200, lr_count: 3\n",
      "* 245 * loss: 5192.862793,  best_loss: 5192.859375, l_rate: 0.000200, lr_count: 4\n",
      "reduce learning rate.. 4.0000001899898055e-05\n",
      "* 246 * loss: 5192.862793,  best_loss: 5192.859375, l_rate: 0.000200, lr_count: 0\n",
      "* 247 * loss: 5192.687988,  best_loss: 5192.687988, l_rate: 0.000040, lr_count: 0\n",
      "* 248 * loss: 5192.680176,  best_loss: 5192.680176, l_rate: 0.000040, lr_count: 0\n",
      "* 249 * loss: 5192.673340,  best_loss: 5192.673340, l_rate: 0.000040, lr_count: 0\n",
      "* 250 * loss: 5192.683594,  best_loss: 5192.673340, l_rate: 0.000040, lr_count: 1\n",
      "* 251 * loss: 5192.674316,  best_loss: 5192.673340, l_rate: 0.000040, lr_count: 2\n",
      "* 252 * loss: 5192.666992,  best_loss: 5192.666992, l_rate: 0.000040, lr_count: 0\n",
      "* 253 * loss: 5192.661621,  best_loss: 5192.661621, l_rate: 0.000040, lr_count: 0\n",
      "* 254 * loss: 5192.671875,  best_loss: 5192.661621, l_rate: 0.000040, lr_count: 1\n",
      "* 255 * loss: 5192.671387,  best_loss: 5192.661621, l_rate: 0.000040, lr_count: 2\n",
      "* 256 * loss: 5192.656250,  best_loss: 5192.656250, l_rate: 0.000040, lr_count: 0\n",
      "* 257 * loss: 5192.675293,  best_loss: 5192.656250, l_rate: 0.000040, lr_count: 1\n",
      "* 258 * loss: 5192.671875,  best_loss: 5192.656250, l_rate: 0.000040, lr_count: 2\n",
      "* 259 * loss: 5192.660156,  best_loss: 5192.656250, l_rate: 0.000040, lr_count: 3\n",
      "* 260 * loss: 5192.671875,  best_loss: 5192.656250, l_rate: 0.000040, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 261 * loss: 5192.668945,  best_loss: 5192.656250, l_rate: 0.000040, lr_count: 0\n",
      "* 262 * loss: 5192.628906,  best_loss: 5192.628906, l_rate: 0.000010, lr_count: 0\n",
      "* 263 * loss: 5192.627930,  best_loss: 5192.627930, l_rate: 0.000010, lr_count: 0\n",
      "* 264 * loss: 5192.626465,  best_loss: 5192.626465, l_rate: 0.000010, lr_count: 0\n",
      "* 265 * loss: 5192.625000,  best_loss: 5192.625000, l_rate: 0.000010, lr_count: 0\n",
      "* 266 * loss: 5192.625488,  best_loss: 5192.625000, l_rate: 0.000010, lr_count: 1\n",
      "* 267 * loss: 5192.616699,  best_loss: 5192.616699, l_rate: 0.000010, lr_count: 0\n",
      "* 268 * loss: 5192.623535,  best_loss: 5192.616699, l_rate: 0.000010, lr_count: 1\n",
      "* 269 * loss: 5192.623535,  best_loss: 5192.616699, l_rate: 0.000010, lr_count: 2\n",
      "* 270 * loss: 5192.620605,  best_loss: 5192.616699, l_rate: 0.000010, lr_count: 3\n",
      "* 271 * loss: 5192.609375,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 0\n",
      "* 272 * loss: 5192.619141,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 1\n",
      "* 273 * loss: 5192.612305,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 2\n",
      "* 274 * loss: 5192.611816,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 3\n",
      "* 275 * loss: 5192.623047,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 276 * loss: 5192.617676,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 0\n",
      "* 277 * loss: 5192.627441,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 1\n",
      "* 278 * loss: 5192.621582,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 2\n",
      "* 279 * loss: 5192.610840,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 3\n",
      "* 280 * loss: 5192.621094,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 281 * loss: 5192.609375,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 0\n",
      "* 282 * loss: 5192.610352,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 1\n",
      "* 283 * loss: 5192.610840,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 2\n",
      "* 284 * loss: 5192.616211,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 3\n",
      "* 285 * loss: 5192.614258,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 286 * loss: 5192.619629,  best_loss: 5192.609375, l_rate: 0.000010, lr_count: 0\n",
      "* 287 * loss: 5192.607422,  best_loss: 5192.607422, l_rate: 0.000010, lr_count: 0\n",
      "* 288 * loss: 5192.613770,  best_loss: 5192.607422, l_rate: 0.000010, lr_count: 1\n",
      "* 289 * loss: 5192.605469,  best_loss: 5192.605469, l_rate: 0.000010, lr_count: 0\n",
      "* 290 * loss: 5192.611816,  best_loss: 5192.605469, l_rate: 0.000010, lr_count: 1\n",
      "* 291 * loss: 5192.610352,  best_loss: 5192.605469, l_rate: 0.000010, lr_count: 2\n",
      "* 292 * loss: 5192.601074,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 0\n",
      "* 293 * loss: 5192.615234,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 1\n",
      "* 294 * loss: 5192.612305,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 2\n",
      "* 295 * loss: 5192.617676,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 3\n",
      "* 296 * loss: 5192.610352,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 297 * loss: 5192.611816,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 0\n",
      "* 298 * loss: 5192.608398,  best_loss: 5192.601074, l_rate: 0.000010, lr_count: 1\n",
      "* 299 * loss: 5192.600098,  best_loss: 5192.600098, l_rate: 0.000010, lr_count: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 300 * loss: 5192.611816,  best_loss: 5192.600098, l_rate: 0.000010, lr_count: 1\n",
      "* 301 * loss: 5192.604980,  best_loss: 5192.600098, l_rate: 0.000010, lr_count: 2\n",
      "* 302 * loss: 5192.606445,  best_loss: 5192.600098, l_rate: 0.000010, lr_count: 3\n",
      "* 303 * loss: 5192.603516,  best_loss: 5192.600098, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 304 * loss: 5192.609375,  best_loss: 5192.600098, l_rate: 0.000010, lr_count: 0\n",
      "* 305 * loss: 5192.599121,  best_loss: 5192.599121, l_rate: 0.000010, lr_count: 0\n",
      "* 306 * loss: 5192.606934,  best_loss: 5192.599121, l_rate: 0.000010, lr_count: 1\n",
      "* 307 * loss: 5192.596680,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 0\n",
      "* 308 * loss: 5192.604492,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 1\n",
      "* 309 * loss: 5192.599121,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 2\n",
      "* 310 * loss: 5192.600098,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 3\n",
      "* 311 * loss: 5192.602051,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 312 * loss: 5192.605957,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 0\n",
      "* 313 * loss: 5192.604004,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 1\n",
      "* 314 * loss: 5192.604980,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 2\n",
      "* 315 * loss: 5192.603516,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 3\n",
      "* 316 * loss: 5192.600586,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 317 * loss: 5192.601074,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 0\n",
      "* 318 * loss: 5192.605957,  best_loss: 5192.596680, l_rate: 0.000010, lr_count: 1\n",
      "* 319 * loss: 5192.584961,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 320 * loss: 5192.600098,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 321 * loss: 5192.601562,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 322 * loss: 5192.604980,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 323 * loss: 5192.601562,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 324 * loss: 5192.595703,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 325 * loss: 5192.604004,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 326 * loss: 5192.595215,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 327 * loss: 5192.593262,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 328 * loss: 5192.596191,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 329 * loss: 5192.601074,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 330 * loss: 5192.598633,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 331 * loss: 5192.600098,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 332 * loss: 5192.593262,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 333 * loss: 5192.585449,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 334 * loss: 5192.594727,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 335 * loss: 5192.599121,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 336 * loss: 5192.599121,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 337 * loss: 5192.597656,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 338 * loss: 5192.594238,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 339 * loss: 5192.593262,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 340 * loss: 5192.602539,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 341 * loss: 5192.595703,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 342 * loss: 5192.595703,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 343 * loss: 5192.593262,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 344 * loss: 5192.593262,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 345 * loss: 5192.599609,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 346 * loss: 5192.594238,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 347 * loss: 5192.591797,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 348 * loss: 5192.591797,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 349 * loss: 5192.595703,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 350 * loss: 5192.591797,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 1\n",
      "* 351 * loss: 5192.599121,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 2\n",
      "* 352 * loss: 5192.592285,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 3\n",
      "* 353 * loss: 5192.596680,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 354 * loss: 5192.588379,  best_loss: 5192.584961, l_rate: 0.000010, lr_count: 0\n",
      "* 355 * loss: 5192.582031,  best_loss: 5192.582031, l_rate: 0.000010, lr_count: 0\n",
      "* 356 * loss: 5192.584473,  best_loss: 5192.582031, l_rate: 0.000010, lr_count: 1\n",
      "* 357 * loss: 5192.580566,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 0\n",
      "* 358 * loss: 5192.594238,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 1\n",
      "* 359 * loss: 5192.588379,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 2\n",
      "* 360 * loss: 5192.585449,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 3\n",
      "* 361 * loss: 5192.588867,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 362 * loss: 5192.587402,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 0\n",
      "* 363 * loss: 5192.596680,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 1\n",
      "* 364 * loss: 5192.587402,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 2\n",
      "* 365 * loss: 5192.586426,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 3\n",
      "* 366 * loss: 5192.584961,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 367 * loss: 5192.590820,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 0\n",
      "* 368 * loss: 5192.581055,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 1\n",
      "* 369 * loss: 5192.587891,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 2\n",
      "* 370 * loss: 5192.587402,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 3\n",
      "* 371 * loss: 5192.582520,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 372 * loss: 5192.583496,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 0\n",
      "* 373 * loss: 5192.580566,  best_loss: 5192.580566, l_rate: 0.000010, lr_count: 1\n",
      "* 374 * loss: 5192.578125,  best_loss: 5192.578125, l_rate: 0.000010, lr_count: 0\n",
      "* 375 * loss: 5192.574707,  best_loss: 5192.574707, l_rate: 0.000010, lr_count: 0\n",
      "* 376 * loss: 5192.575195,  best_loss: 5192.574707, l_rate: 0.000010, lr_count: 1\n",
      "* 377 * loss: 5192.587402,  best_loss: 5192.574707, l_rate: 0.000010, lr_count: 2\n",
      "* 378 * loss: 5192.570312,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 0\n",
      "* 379 * loss: 5192.582520,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 1\n",
      "* 380 * loss: 5192.588867,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 2\n",
      "* 381 * loss: 5192.577148,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 3\n",
      "* 382 * loss: 5192.579102,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 383 * loss: 5192.579590,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 0\n",
      "* 384 * loss: 5192.580078,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 1\n",
      "* 385 * loss: 5192.580566,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 2\n",
      "* 386 * loss: 5192.585938,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 3\n",
      "* 387 * loss: 5192.576172,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 388 * loss: 5192.573730,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 0\n",
      "* 389 * loss: 5192.578613,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 1\n",
      "* 390 * loss: 5192.574707,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 2\n",
      "* 391 * loss: 5192.573242,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 3\n",
      "* 392 * loss: 5192.579590,  best_loss: 5192.570312, l_rate: 0.000010, lr_count: 4\n",
      "* 393 * loss: 5192.568848,  best_loss: 5192.568848, l_rate: 0.000010, lr_count: 0\n",
      "* 394 * loss: 5192.567871,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 395 * loss: 5192.576660,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 1\n",
      "* 396 * loss: 5192.572266,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 2\n",
      "* 397 * loss: 5192.580566,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 3\n",
      "* 398 * loss: 5192.571777,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 399 * loss: 5192.568848,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 0\n",
      "* 400 * loss: 5192.573242,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 1\n",
      "* 401 * loss: 5192.575684,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 2\n",
      "* 402 * loss: 5192.580566,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 3\n",
      "* 403 * loss: 5192.571777,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 404 * loss: 5192.576172,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 0\n",
      "* 405 * loss: 5192.573242,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 1\n",
      "* 406 * loss: 5192.580078,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 2\n",
      "* 407 * loss: 5192.574219,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 3\n",
      "* 408 * loss: 5192.575195,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 409 * loss: 5192.572266,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 0\n",
      "* 410 * loss: 5192.571289,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 1\n",
      "* 411 * loss: 5192.574707,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 2\n",
      "* 412 * loss: 5192.572754,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 3\n",
      "* 413 * loss: 5192.574219,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 414 * loss: 5192.580078,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 0\n",
      "* 415 * loss: 5192.568848,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 1\n",
      "* 416 * loss: 5192.573730,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 2\n",
      "* 417 * loss: 5192.570312,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 3\n",
      "* 418 * loss: 5192.573730,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 419 * loss: 5192.572754,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 0\n",
      "* 420 * loss: 5192.576172,  best_loss: 5192.567871, l_rate: 0.000010, lr_count: 1\n",
      "* 421 * loss: 5192.562988,  best_loss: 5192.562988, l_rate: 0.000010, lr_count: 0\n",
      "* 422 * loss: 5192.565430,  best_loss: 5192.562988, l_rate: 0.000010, lr_count: 1\n",
      "* 423 * loss: 5192.574219,  best_loss: 5192.562988, l_rate: 0.000010, lr_count: 2\n",
      "* 424 * loss: 5192.559570,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 0\n",
      "* 425 * loss: 5192.568848,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 1\n",
      "* 426 * loss: 5192.566406,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 2\n",
      "* 427 * loss: 5192.560059,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 3\n",
      "* 428 * loss: 5192.562012,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 429 * loss: 5192.571777,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 0\n",
      "* 430 * loss: 5192.568848,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 1\n",
      "* 431 * loss: 5192.569824,  best_loss: 5192.559570, l_rate: 0.000010, lr_count: 2\n",
      "* 432 * loss: 5192.556641,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 0\n",
      "* 433 * loss: 5192.558594,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 1\n",
      "* 434 * loss: 5192.568359,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 2\n",
      "* 435 * loss: 5192.567871,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 3\n",
      "* 436 * loss: 5192.563477,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 437 * loss: 5192.568848,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 0\n",
      "* 438 * loss: 5192.565430,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 1\n",
      "* 439 * loss: 5192.570312,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 2\n",
      "* 440 * loss: 5192.566895,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 3\n",
      "* 441 * loss: 5192.564453,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 442 * loss: 5192.567871,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 0\n",
      "* 443 * loss: 5192.560059,  best_loss: 5192.556641, l_rate: 0.000010, lr_count: 1\n",
      "* 444 * loss: 5192.554688,  best_loss: 5192.554688, l_rate: 0.000010, lr_count: 0\n",
      "* 445 * loss: 5192.563477,  best_loss: 5192.554688, l_rate: 0.000010, lr_count: 1\n",
      "* 446 * loss: 5192.550293,  best_loss: 5192.550293, l_rate: 0.000010, lr_count: 0\n",
      "* 447 * loss: 5192.567871,  best_loss: 5192.550293, l_rate: 0.000010, lr_count: 1\n",
      "* 448 * loss: 5192.566895,  best_loss: 5192.550293, l_rate: 0.000010, lr_count: 2\n",
      "* 449 * loss: 5192.561523,  best_loss: 5192.550293, l_rate: 0.000010, lr_count: 3\n",
      "* 450 * loss: 5192.546875,  best_loss: 5192.546875, l_rate: 0.000010, lr_count: 0\n",
      "* 451 * loss: 5192.555176,  best_loss: 5192.546875, l_rate: 0.000010, lr_count: 1\n",
      "* 452 * loss: 5192.545898,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 0\n",
      "* 453 * loss: 5192.549316,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 1\n",
      "* 454 * loss: 5192.549316,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 2\n",
      "* 455 * loss: 5192.559570,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 3\n",
      "* 456 * loss: 5192.553711,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 457 * loss: 5192.558105,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 0\n",
      "* 458 * loss: 5192.558594,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 1\n",
      "* 459 * loss: 5192.566406,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 2\n",
      "* 460 * loss: 5192.561035,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 3\n",
      "* 461 * loss: 5192.555664,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 462 * loss: 5192.556152,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 0\n",
      "* 463 * loss: 5192.546875,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 1\n",
      "* 464 * loss: 5192.553711,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 2\n",
      "* 465 * loss: 5192.558105,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 3\n",
      "* 466 * loss: 5192.558594,  best_loss: 5192.545898, l_rate: 0.000010, lr_count: 4\n",
      "* 467 * loss: 5192.541504,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 0\n",
      "* 468 * loss: 5192.551270,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 1\n",
      "* 469 * loss: 5192.552734,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 2\n",
      "* 470 * loss: 5192.556152,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 3\n",
      "* 471 * loss: 5192.557617,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 472 * loss: 5192.551270,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 0\n",
      "* 473 * loss: 5192.555176,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 1\n",
      "* 474 * loss: 5192.551270,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 2\n",
      "* 475 * loss: 5192.550293,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 3\n",
      "* 476 * loss: 5192.543457,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 477 * loss: 5192.547363,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 0\n",
      "* 478 * loss: 5192.554199,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 1\n",
      "* 479 * loss: 5192.552734,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 2\n",
      "* 480 * loss: 5192.546875,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 3\n",
      "* 481 * loss: 5192.560547,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 482 * loss: 5192.555664,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 0\n",
      "* 483 * loss: 5192.548340,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 1\n",
      "* 484 * loss: 5192.555176,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 2\n",
      "* 485 * loss: 5192.553711,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 3\n",
      "* 486 * loss: 5192.555664,  best_loss: 5192.541504, l_rate: 0.000010, lr_count: 4\n",
      "* 487 * loss: 5192.541016,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 0\n",
      "* 488 * loss: 5192.549316,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 1\n",
      "* 489 * loss: 5192.561523,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 2\n",
      "* 490 * loss: 5192.543945,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 491 * loss: 5192.552734,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 492 * loss: 5192.547852,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 0\n",
      "* 493 * loss: 5192.548340,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 1\n",
      "* 494 * loss: 5192.541016,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 2\n",
      "* 495 * loss: 5192.549316,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 3\n",
      "* 496 * loss: 5192.547852,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 4\n",
      "reduce learning rate.. 1e-05\n",
      "* 497 * loss: 5192.553711,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 0\n",
      "* 498 * loss: 5192.542480,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 1\n",
      "* 499 * loss: 5192.544922,  best_loss: 5192.541016, l_rate: 0.000010, lr_count: 2\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "# Initialize values\n",
    "best_loss, count = float('inf'), 0\n",
    "\n",
    "# Start epoch loop\n",
    "for epoch in range(epochs):\n",
    "    for inputs, outputs in train_ds:\n",
    "        train_step(inputs)\n",
    "    \n",
    "    # Get loss and leraning rate at this epoch\n",
    "    t_loss = train_loss.result().numpy() \n",
    "    l_rate = optimizer.learning_rate.numpy()\n",
    "\n",
    "    # Control learning rate\n",
    "    count, lr  = reduce_lr(best_loss, t_loss, count, l_rate, 5, 0.2, 0.00001)\n",
    "    optimizer.learning_rate = lr\n",
    "    \n",
    "    # Save checkpoint if best v_loss \n",
    "    if t_loss < best_loss:\n",
    "        best_loss = t_loss\n",
    "        checkpoint.save(file_prefix=os.path.join(save_path+'/ckp/', 'ckp'))\n",
    "    \n",
    "    # Save loss, lerning rate\n",
    "    print(\"* %i * loss: %f,  best_loss: %f, l_rate: %f, lr_count: %i\"%(epoch, t_loss, best_loss, l_rate, count ))\n",
    "    df = pd.DataFrame({'epoch':[epoch], 'loss':[t_loss], 'best_loss':[best_loss], 'l_rate':[l_rate]  } )\n",
    "    df.to_csv(save_path+'/process.csv', mode='a', header=False)\n",
    "    \n",
    "    # Reset loss\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
